{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feb2f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import mannwhitneyu\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "product = \"GeschirrspÃ¼ler\"\n",
    "\n",
    "df = pd.read_csv(f\"{product}/{product}_results_per_model.csv\", sep=\",\", dtype={\"Vendor\": str})\n",
    "#df = pd.read_csv(f\"{product}/{product}_first_timepoint_results.csv\", sep=\",\", dtype={\"Vendor\": str}) # uncomment if only first timepoint should be analyzed\n",
    "\n",
    "df_original = df.copy()\n",
    "#df = df[(df['Manufacturer'] != \"Miele\") & (df['Manufacturer'] != \"Liebherr\")]\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d028223",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude num_price_changes for clustering. Reasons: feature has high specificity but low sensitivity. Also, data spanning a short time span could make this feature unreliable\n",
    "features = df[[\"mean_coef_var\", \"same_price_pct\", \"cheapest_same_price_pct\", \"entropy\"]]\n",
    "\n",
    "#features = df[[\"mean_coef_var\"]] # uncomment if only the coef var should be used for clustering\n",
    "\n",
    "print(features.mean())\n",
    "\n",
    "# Standardizing the features\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4311d77b",
   "metadata": {},
   "source": [
    "**DBSCAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a805274e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ranges for eps and min_samples to find the optimal parameters according to the Silhouette Score\n",
    "eps_range = np.arange(0.1, 1.7, 0.1) # Zwischen 0.1 und 5 getestet: KS: 1, WS: 1.2, GefrierS: 1.1, GeschirrS: 0.7, Lautsprecher: 1.4. Aber: ohne Miele & Liebherr 0.1 optimal bei KS\n",
    "min_samples_range = range(3, len(df)//2, 2) # min_samples cannot be greater than half of samples. Start with 3, since every manufacturer in the remaining data has at least 3 models\n",
    "\n",
    "best_eps = None\n",
    "best_min_samples = None\n",
    "best_score = -1\n",
    "\n",
    "# Find the best eps and min_samples\n",
    "for eps in eps_range:\n",
    "    for min_samples in min_samples_range:\n",
    "        db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = db.fit_predict(scaled_data)\n",
    "        \n",
    "        # Calculate silhouette score if there is more than one cluster\n",
    "        if len(set(labels)) > 1:\n",
    "            score = silhouette_score(scaled_data, labels)\n",
    "            if score > best_score:\n",
    "                best_eps = eps\n",
    "                best_min_samples = min_samples\n",
    "                best_score = score\n",
    "\n",
    "print(f\"Best eps: {best_eps}\")\n",
    "print(f\"Best min_samples: {best_min_samples}\")\n",
    "print(f\"Best Silhouette Score: {best_score}\")\n",
    "\n",
    "# Fit DBSCAN with best eps and min_samples\n",
    "db = DBSCAN(eps=best_eps, min_samples=best_min_samples)\n",
    "labels_dbscan = db.fit_predict(scaled_data)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "df['cluster_dbscan'] = labels_dbscan\n",
    "\n",
    "# display(df[df['Manufacturer'] == 'Siemens'])\n",
    "# display(df[df['Manufacturer'] == 'Miele'])\n",
    "\n",
    "display(df)\n",
    "\n",
    "# Calculate Silhouette Score and print it\n",
    "dbscan_silhouette = silhouette_score(scaled_data, df['cluster_dbscan'])\n",
    "print(f\"Silhouette Score for DBSCAN: {dbscan_silhouette}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a061a596",
   "metadata": {},
   "source": [
    "**Validating results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of detected clusters\n",
    "cluster_labels = df['cluster_dbscan'].unique()\n",
    "print(f\"Number of clusters: {len(cluster_labels)}\")\n",
    "\n",
    "# If only one cluster is detected or more than 4 clusters are detected, there most likely is no RPM\n",
    "if len(cluster_labels) == 1 or len(cluster_labels) > 9:\n",
    "    print(\"No RPM has been detected\")\n",
    "else:\n",
    "    # Create dict with cluster as key and 25th percentile coef_var as value. Reason: If the outlier cluster has at least 25% low outliers, we correctly identify it\n",
    "    cluster_coefvar = {}\n",
    "    for cluster in cluster_labels:\n",
    "        cluster_coefvar[cluster] = df[df['cluster_dbscan'] == cluster]['mean_coef_var'].quantile(0.25)\n",
    "    \n",
    "    print(cluster_coefvar)\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    \n",
    "    # Get the clusters with the lowest and second lowest coef_var\n",
    "    sorted_clusters = sorted(cluster_coefvar.items(), key=lambda x: x[1])\n",
    "    lowest = sorted_clusters[0][0]\n",
    "    second_lowest = sorted_clusters[1][0]\n",
    "\n",
    "    # Make lowest cluster the suspicious cluster and second lowest the normal cluster\n",
    "    outliers = df[df['cluster_dbscan'] == lowest]\n",
    "    non_sus = df[df['cluster_dbscan'] == second_lowest]\n",
    "\n",
    "    # Define fixed threshold as safety measure, since otherwise low outliers, which are only suspicious compared to the other products, are flagged\n",
    "    non_sus_same_price_cutoff = 0.65\n",
    "    sus_cheapest_same_price_cutoff = 0.85\n",
    "\n",
    "    # Just in case: Only keep low outliers in, remove high outliers from suspicious cluster, if there are any (normally not the case)\n",
    "    print(f\"These were removed from the suspicious cluster via the Same price cutoff ({non_sus_same_price_cutoff})\")\n",
    "    sus_only = outliers[outliers['same_price_pct'] >= non_sus_same_price_cutoff]\n",
    "    display(outliers[outliers['same_price_pct'] < non_sus_same_price_cutoff])\n",
    "\n",
    "    # Just in case: Add definitely suspicious models, which were somehow not part of the suspicious cluster\n",
    "    df_cheap_threshold = df[df['cheapest_same_price_pct'] > 0.85]\n",
    "    sus_only = pd.concat([sus_only, df_cheap_threshold])\n",
    "    sus_only = sus_only.drop_duplicates()\n",
    "\n",
    "    #display(sus_only.sort_values(by='mean_coef_var', ascending=False))\n",
    "\n",
    "    alpha_level = 0.05\n",
    "\n",
    "    # Initialize confidence variable, which gets higher if more tests are passed\n",
    "    confidence = 0\n",
    "\n",
    "    # Coef_var Mann-Whitney-U test: Mann-Whitney-U test because we have non-normal data and unequal variances\n",
    "    print(f\"Average coef var in sus cluster: {sus_only['mean_coef_var'].mean()}\")\n",
    "    print(f\"Average coef var in normal cluster: {non_sus['mean_coef_var'].mean()}\")\n",
    "\n",
    "    sus_cluster_coefvar = sus_only['mean_coef_var']\n",
    "    normal_cluster_coefvar = non_sus['mean_coef_var']\n",
    "\n",
    "    t_coefvar, p_coefvar = mannwhitneyu(sus_cluster_coefvar, normal_cluster_coefvar, alternative='less') # One sided\n",
    "\n",
    "    print(f\"P-value for coef-var: {round(p_coefvar, 2)} ({p_coefvar})\")\n",
    "\n",
    "    if p_coefvar < alpha_level:\n",
    "        rpm_detected = True\n",
    "        print(\"Result: One cluster has significantly lower variation in prices than the other(s)\")\n",
    "    else:\n",
    "        print(\"Result: Both clusters have similar variation in prices\")\n",
    "\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    \n",
    "    # Num_price_changes Mann-Whitney-U test ---------------------------------------------\n",
    "\n",
    "    if 'num_price_changes' in df.columns:\n",
    "        print(f\"Average num_price_changes in sus cluster: {sus_only['num_price_changes'].mean()}\")\n",
    "        print(f\"Average num_price_changes in normal cluster: {non_sus['num_price_changes'].mean()}\")\n",
    "\n",
    "        if sus_only['num_price_changes'].mean() >= non_sus['num_price_changes'].mean():\n",
    "            print(\"Result: The suspicious cluster has actually a higher number of price changes than the normal cluster\")\n",
    "        else:\n",
    "            sus_cluster_price_changes = sus_only['num_price_changes']\n",
    "            normal_cluster_price_changes = non_sus['num_price_changes']\n",
    "\n",
    "            t_changes, p_changes = mannwhitneyu(sus_cluster_price_changes, normal_cluster_price_changes, alternative='less') # One sided\n",
    "\n",
    "            print(f\"P-value for price changes: {round(p_changes, 2)} ({p_changes})\")\n",
    "\n",
    "            if p_changes < alpha_level:\n",
    "                confidence += 1\n",
    "                print(\"Result: The same cluster also has significantly less price_changes than the other(s). Cluster likely represents RPM\")\n",
    "            else:\n",
    "                print(\"Result: The same cluster DOES NOT have significantly less price_changes than the other\")\n",
    "            \n",
    "            #display(sus_only[(sus_only['Manufacturer'] != \"Miele\") | (sus_only['Manufacturer'] != \"Liebherr\")].sort_values(by='mean_coef_var', ascending=False))\n",
    "            #display(non_sus[(non_sus['Manufacturer'] == \"Miele\") | (non_sus['Manufacturer'] == \"Liebherr\")].sort_values(by='mean_coef_var', ascending=True))\n",
    "    else:\n",
    "        print(\"Time span too short to validate RPM cluster using num_price_changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fddfef9",
   "metadata": {},
   "source": [
    "**Aggregate per Manufacturer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c59641",
   "metadata": {},
   "outputs": [],
   "source": [
    "if rpm_detected:\n",
    "    # Get the number of models per manufacturer\n",
    "    value_counts = df['Manufacturer'].value_counts().reset_index()\n",
    "    value_counts.columns = ['Manufacturer', 'total_models']\n",
    "    #display(value_counts)\n",
    "\n",
    "    # Group by manufacturer and count occurrences\n",
    "    sus = sus_only.groupby('Manufacturer').size().reset_index(name='sus_count')\n",
    "    #display(sus)\n",
    "\n",
    "    # Merge the DataFrames\n",
    "    result = pd.merge(value_counts, sus, on='Manufacturer', how='left').fillna(0)\n",
    "    result['sus_count'] = result['sus_count'].astype(int)\n",
    "    result['pct_suspicious_models'] = round(result['sus_count'] / result['total_models'] * 100, 2)\n",
    "    result = result.sort_values(by=['pct_suspicious_models', 'sus_count'], ascending=False)\n",
    "\n",
    "    total_sus_models = result['sus_count'].sum()\n",
    "    result['pct_of_sus_cluster'] = round(result['sus_count'] / total_sus_models * 100, 2)\n",
    "\n",
    "    # Only suspicious manufacturers (at least 5 sus models and more than 50 % sus models)\n",
    "    sus_manufacturers = result[(result['sus_count'] > 4) & (result['pct_suspicious_models'] > 40)]\n",
    "\n",
    "    if len(sus_manufacturers) == 0:\n",
    "        print(\"No suspicious manufacturers found.\")\n",
    "    else:\n",
    "        print(\"Suspicious manufacturers (at least 5 suspicious models and more than 40% of all models being suspicious)\")\n",
    "        display(sus_manufacturers)\n",
    "\n",
    "    print(\"Full result of analysis\")\n",
    "    result.rename(columns={\"total_models\": \"total_products\", \"sus_count\": \"suspicious_products\", \"pct_suspicious_models\": \"%_suspicious_products\", \"pct_of_sus_cluster\": \"%_of_suspicious_cluster\"}, inplace=True)\n",
    "    display(result)\n",
    "    # change\n",
    "    result.to_csv(f\"{product}/{product}_sus_manufacturers_coefvar.csv\", index=False)\n",
    "    \n",
    "    print(f\"There are {len(sus_only)} suspicious models, which is {len(sus_only) / len(df) * 100:.2f}% of the total models.\")\n",
    "    if confidence > 0:\n",
    "        print(\"High confidence that the marked models are indicating RPM.\")\n",
    "    else:\n",
    "        print(\"Medium confidence that the marked models are indicating RPM.\")\n",
    "else:\n",
    "    print(\"No RPM has been detected.\")\n",
    "\n",
    "display(sus_only.sort_values(by='entropy', ascending=False))\n",
    "\n",
    "sus_cluster_label = sus_only['cluster_dbscan'].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5acdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge on both 'Model' and 'mean_coef_var'\n",
    "df_merged = pd.merge(df_original, sus_only, on=['Model', 'mean_coef_var', 'num_price_changes', 'same_price_pct', 'cheapest_same_price_pct', 'entropy'], how='left', indicator=True)\n",
    "\n",
    "# Create 'suspicious' column: 'True' if combination exists in smaller df, otherwise 'No'\n",
    "df_original['suspicious'] = np.where(df_merged['_merge'] == 'both', True, False)\n",
    "\n",
    "display(df_original)\n",
    "print(df_original['suspicious'].value_counts()[True])\n",
    "\n",
    "# Save result to csv\n",
    "df_original.to_csv(f\"{product}/{product}_suspicious.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8acdea71",
   "metadata": {},
   "source": [
    "**Visualizing clusters using PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1092390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(scaled_data)\n",
    "\n",
    "print(X_pca)\n",
    "\n",
    "# Custom legend mapping for the clusters\n",
    "custom_labels = {}\n",
    "non_suspicious_counter = 0  # Counter to assign colors to non-suspicious clusters\n",
    "\n",
    "unique_labels = np.unique(labels_dbscan)\n",
    "\n",
    "# Define darker, less bright colors for non-suspicious clusters\n",
    "non_suspicious_colors = ['#4a6fa5', '#468966', '#ad5ead']  # Blue, green, magenta\n",
    "cluster_colors = {}\n",
    "\n",
    "for label in unique_labels:\n",
    "    if label == sus_cluster_label:\n",
    "        custom_labels[label] = \"Suspicious cluster\"\n",
    "        cluster_colors[label] = 'orange'  # Orange for the suspicious cluster\n",
    "    else:\n",
    "        custom_labels[label] = f\"Non Suspicious cluster {non_suspicious_counter + 1}\"\n",
    "        cluster_colors[label] = non_suspicious_colors[non_suspicious_counter]\n",
    "        non_suspicious_counter += 1\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot each cluster with the assigned color and custom label\n",
    "for label in unique_labels:\n",
    "    label_mask = labels_dbscan == label\n",
    "    plt.scatter(X_pca[label_mask, 0], X_pca[label_mask, 1], \n",
    "                label=custom_labels[label], color=cluster_colors[label])\n",
    "\n",
    "product_eng = {\n",
    "    \"Waschmaschinen\": \"Washing Machines\",\n",
    "    \"GeschirrspÃ¼ler\": \"Dishwashers\",\n",
    "    \"GefrierschrÃ¤nke\": \"Freezers\",\n",
    "    \"KÃ¼hlschrÃ¤nke\": \"Refrigerators\",\n",
    "    \"Lautsprecher\": \"Loudspeakers\"\n",
    "}\n",
    "\n",
    "plt.title(f'DBSCAN Clustering with PCA Reduction: {product_eng[product]}')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
